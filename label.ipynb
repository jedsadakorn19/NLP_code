{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "label.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jedsadakorn19/NLP_code/blob/main/label.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS4eAytjavwv"
      },
      "source": [
        "!pip3 install pythainlp\r\n",
        "!pip install \"tensorflow>=1.9,<2.0\"\r\n",
        "!pip install --upgrade tensorflow-hub\r\n",
        "!pip install bert-for-tf2\r\n",
        "!pip install sklearn_crfsuite\r\n",
        "!pip install https://github.com/PyThaiNLP/pythainlp/archive/dev.zip\r\n",
        "!pip install fastai==1.0.46\r\n",
        "!pip install emoji\r\n",
        "!pip install transformers\r\n",
        "!pip install \"sentencepiece==0.0.9\"\r\n",
        "!pip3 install tf-sentencepiece\r\n",
        "# !pip install tensorflow-hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFdtlbJHL4Jr"
      },
      "source": [
        "import pythainlp\r\n",
        "pythainlp.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9q_JTydbZSv"
      },
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "!git clone https://github.com/thirasan/bert-1.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMVdQfFTabJ4"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "import codecs\n",
        "import re,urllib\n",
        "import numpy as np\n",
        "from pythainlp.tokenize import word_tokenize,Tokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from bert import bert_tokenization\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from pythainlp.tokenize import Tokenizer\n",
        "from pythainlp.util.trie import Trie, dict_trie\n",
        "from pythainlp.corpus.common import thai_words\n",
        "from pythainlp.corpus import thai_stopwords\n",
        "from tqdm import tqdm_notebook\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from IPython.display import Image \n",
        "import datetime\n",
        "from sklearn.metrics import recall_score, precision_score, classification_report, accuracy_score, confusion_matrix, f1_score\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "import dill as pickle\n",
        "from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pythainlp import word_vector\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "model_path = 'thwiki_data/models/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2WYKJ38nhSd"
      },
      "source": [
        "model = AutoModel.from_pretrained(\"monsoon-nlp/bert-base-thai\")\r\n",
        "# model = word_vector.get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cufaiABabJ4"
      },
      "source": [
        "file_name=\"1702\" # ชื่อไฟล์คลังข้อมูล\n",
        "def get_data(fileopen):\n",
        "\t\"\"\"\n",
        "    สำหรับใช้อ่านทั้งหมดทั้งในไฟล์ทีละรรทัดออกมาเป็น list\n",
        "    \"\"\"\n",
        "\twith codecs.open(fileopen, 'r',encoding='utf-8-sig') as f:\n",
        "\t\tlines = f.read().splitlines()\n",
        "\treturn [a for a in lines if Unique(a)]# เอาไม่ซ้ำกัน"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_DB-L3XabJ4"
      },
      "source": [
        "data_not=[]\n",
        "def Unique(p):\n",
        " text=re.sub(\"<[^>]*>\",\"\",p)\n",
        " text=re.sub(\"\\[(.*?)\\]\",\"\",text)\n",
        " text=re.sub(\"\\[\\/(.*?)\\]\",\"\",text)   \n",
        " if text not in data_not:\n",
        "  data_not.append(text)\n",
        "  return True\n",
        " else:\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35hwJUBeabJ4"
      },
      "source": [
        "rawdata = get_data(\"1702.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiYGVsWjabJ4"
      },
      "source": [
        "print(len(rawdata))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4deB7nUYabJ5"
      },
      "source": [
        "def toolner_to_tag(text):\n",
        " text=text.strip()\n",
        " text=re.sub(\"<[^>]*>\",\"\",text)\n",
        " text=re.sub(\"(\\[\\/(.*?)\\])\",\"\\\\1***\",text)#.replace('(\\[(.*?)\\])','***\\\\1')#  ตัดการกับพวกไม่มี tag word\n",
        " text=re.sub(\"(\\[\\w+\\])\",\"***\\\\1\",text)\n",
        " text2=[]\n",
        " for i in text.split('***'):\n",
        "  if \"[\" in i:\n",
        "   text2.append(i)\n",
        "  # else:\n",
        "  #  text2.append(\"[word]\"+i+\"[/word]\")\n",
        " text=\"\".join(text2)#re.sub(\"[word][/word]\",\"\",\"\".join(text2))\n",
        " return text.replace(\"[word][/word]\",\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aGE-4t9abJ5"
      },
      "source": [
        "def text2conll2002(text,pos=True):\n",
        "    text=toolner_to_tag(text) # นำไปใส่ tag [word] \"[word]เราจะไปเดินเล่นที่[/word]\"\n",
        "    text=text.replace(\"''\",'\"')\n",
        "    text=text.replace(\"(\",'')\n",
        "    text=text.replace(\")\",'')\n",
        "    text=text.replace(\"[Pronouns]\",'')\n",
        "    text=text.replace(\"[noun]\",'')\n",
        "    text=text.replace(\"[intransitive verb]\",'')\n",
        "    text=text.replace(\"[transitive verb]\",'')\n",
        "    text=text.replace(\"[Denial word]\",'')\n",
        "    text=text.replace(\"[word before verb]\",'')\n",
        "    text=text.replace(\"[word after verb]\",'')\n",
        "    text=text.replace(\"[Adverbs]\",'')\n",
        "    text=text.replace(\"[Adverb]\",'')\n",
        "    text=text.replace(\"[Qualitative noun]\",'')\n",
        "    text=text.replace(\"[Adjectives]\",'')\n",
        "    text=text.replace(\"[Numeral word]\",'')\n",
        "    text=text.replace(\"[Sequence word]\",'')\n",
        "    text=text.replace(\"[word before numeral]\",'')\n",
        "    text=text.replace(\"[word after numeral]\",'')\n",
        "    text=text.replace(\"[Time expression word]\",'')\n",
        "    text=text.replace(\"[Conclusion word]\",'')\n",
        "    text=text.replace(\"[Special word]\",'')\n",
        "    text=text.replace(\"[Prepositions]\",'')\n",
        "    text=text.replace(\"[Conjunctions]\",'')\n",
        "    text=text.replace(\"[SS Conjunctions]\",'')\n",
        "    text=text.replace(\"[Interjections]\",'')\n",
        "    text=text.replace(\"[abbreviation]\",'')\n",
        "    text=text.replace(\"[word before noun]\",'')\n",
        "    text=text.replace(\"[Phrasal Verbs]\",'')\n",
        "    text=text.replace(\"[Noun phrase]\",'')\n",
        "    text=text.replace(\"[Noun Conjunctions]\",'')\n",
        "    text=text.replace(\"[Negator]\",'')\n",
        "    text=text.replace(\"’\",'\"').replace(\"‘\",'\"')#.replace('\"',\"\")\n",
        "    tag=tokenizer1.tokenize(text) # แยก tag ออกมาจากข้อความ [('word', 'ฉันจะไปเดินเล่นที่', 'word'),('LOCATION', 'หนองคาย', 'LOCATION'),('word', 'พร้อมกับนั่งเรือข้ามไป', 'word'),('LOCATION', 'ประเทศลาว', 'LOCATION')]\n",
        "    return tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbkyeBtJabJ5"
      },
      "source": [
        "pattern = '(.*?)\\[\\/(.*?)\\]'\n",
        "tokenizer1 = RegexpTokenizer(pattern) # ใช้ nltk.tokenize.RegexpTokenizer เพื่อตัด [TIME]8.00[/TIME] ให้เป็น ('TIME','ไง','TIME')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FsRRURWabJ5"
      },
      "source": [
        "text2conll2002(rawdata[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_muUMosiQ7iK"
      },
      "source": [
        "print((rawdata[65]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNd4gqGnabJ5"
      },
      "source": [
        "d=[]\n",
        "o=[]\n",
        "for i in range(len(rawdata[0:75])):\n",
        "    r=text2conll2002(rawdata[i])\n",
        "    d.append(r)\n",
        "for i in range(76,90):\n",
        "    m=text2conll2002(rawdata[i])\n",
        "    o.append(m)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJJuT2bPHg_e"
      },
      "source": [
        "o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQQeL_rzj-_0"
      },
      "source": [
        "d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVOlARavSzqY"
      },
      "source": [
        "train_sentences = d\r\n",
        "test_sentences  = o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDNgAGjpabJ5"
      },
      "source": [
        "print(len(train_sentences))\n",
        "print(\"Tagged words in train set:\", len([item for sublist in train_sentences for item in sublist]))\n",
        "print(len(test_sentences))\n",
        "print(\"Tagged words in test set:\", len([item for sublist in test_sentences for item in sublist]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJdHCQRYabJ5"
      },
      "source": [
        "def tag_sequence(sentences):\n",
        "    return [[t for w, t in sentence] for sentence in sentences]\n",
        "\n",
        "def text_sequence(sentences):\n",
        "    return [[w for w, t in sentence] for sentence in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbT8xxKfczBw"
      },
      "source": [
        "tags1 = set([item for sublist in test_sentences for _, item in sublist])\r\n",
        "tags2 = set([item for sublist in train_sentences for _, item in sublist])\r\n",
        "print('TOTAL TAGS: ', len(tags1))\r\n",
        "print('TOTAL TAGS: ', len(tags2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGk2PzrSabJ5"
      },
      "source": [
        "tags = set([item for sublist in train_sentences+test_sentences for _, item in sublist])\n",
        "print('TOTAL TAGS: ', len(tags))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_28DwOPkOx6"
      },
      "source": [
        "print(tags1)\r\n",
        "print(tags2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvYSU0GDabJ5"
      },
      "source": [
        "print(tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el-qYWk0abJ5"
      },
      "source": [
        "tag2int = {}\n",
        "int2tag = {}\n",
        "\n",
        "for i, tag in enumerate(sorted(tags)):\n",
        "    tag2int[tag] = i+1\n",
        "    int2tag[i+1] = tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okaIYQ3BY5pp"
      },
      "source": [
        "tag2int['-PAD-'] = 0\n",
        "int2tag[0] = '-PAD-'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XGyu2IeY7ig"
      },
      "source": [
        "n_tags = len(tag2int)\n",
        "print('Total tags:', n_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giV6qFxKY9rL"
      },
      "source": [
        "print(tag2int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_G5d_I0ZV8p"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 70\n",
        "EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnVnt1yGZWWp"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.hist([len(s) for s in train_sentences], bins=50)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xxs_HBPZnI5"
      },
      "source": [
        "print('Max sentence length:',len(max(rawdata, key=len)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzOtADixZqGZ"
      },
      "source": [
        "def  split(sentences, max):\n",
        "    new=[]\n",
        "    for data in sentences:\n",
        "        new.append(([data[x:x+max] for x in range(0, len(data), max)]))\n",
        "    new = [val for sublist in new for val in sublist]\n",
        "    return new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ_6mMsCZyGg"
      },
      "source": [
        "train_sentences = split(train_sentences, MAX_SEQUENCE_LENGTH)\r\n",
        "test_sentences  = split(test_sentences, MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlfYx9NzJBvP"
      },
      "source": [
        "test_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D1dx0xYHENP"
      },
      "source": [
        "train_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6WFkuKwZ0VQ"
      },
      "source": [
        "len(max(train_sentences, key=len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KlTgXPWZ53B"
      },
      "source": [
        "# Initialize session\n",
        "sess = tf.compat.v1.Session()\n",
        "# Params for bert model and tokenization\n",
        "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ_NLJGkblgr"
      },
      "source": [
        "train_text = text_sequence(train_sentences)\r\n",
        "train_label = tag_sequence(train_sentences)\r\n",
        "test_text = text_sequence(test_sentences)\r\n",
        "test_label= tag_sequence(test_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwatamUqxKyC"
      },
      "source": [
        "len(test_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_y2cvqcxMLT"
      },
      "source": [
        "train_text[0], train_label[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMmQVAzlxnXs"
      },
      "source": [
        "class PaddingInputExample(object):\r\n",
        "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\r\n",
        "  When running eval/predict on the TPU, we need to pad the number of examples\r\n",
        "  to be a multiple of the batch size, because the TPU requires a fixed batch\r\n",
        "  size. The alternative is to drop the last batch, which is bad because it means\r\n",
        "  the entire output data won't be generated.\r\n",
        "  We use this class instead of `None` because treating `None` as padding\r\n",
        "  battches could cause silent errors.\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "class InputExample(object):\r\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\r\n",
        "        \"\"\"Constructs a InputExample.\r\n",
        "    Args:\r\n",
        "      guid: Unique id for the example.\r\n",
        "      text_a: string. The untokenized text of the first sequence. For single\r\n",
        "        sequence tasks, only this sequence must be specified.\r\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\r\n",
        "        Only must be specified for sequence pair tasks.\r\n",
        "      label: (Optional) string. The label of the example. This should be\r\n",
        "        specified for train and dev examples, but not for test examples.\r\n",
        "    \"\"\"\r\n",
        "        self.guid = guid\r\n",
        "        self.text_a = text_a\r\n",
        "        self.text_b = text_b\r\n",
        "        self.label = label\r\n",
        "\r\n",
        "def create_tokenizer_from_hub_module():\r\n",
        "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\r\n",
        "    bert_module =  hub.Module(bert_path)\r\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\r\n",
        "    # bert_module =  hub.load(bert_path)\r\n",
        "    # tokenization_info = bert_module.signatures['tokenization_info']\r\n",
        "    vocab_file, do_lower_case = sess.run(\r\n",
        "        [\r\n",
        "            tokenization_info[\"vocab_file\"],\r\n",
        "            tokenization_info[\"do_lower_case\"],\r\n",
        "        ]\r\n",
        "    )\r\n",
        "\r\n",
        "    return  bert_tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\r\n",
        "\r\n",
        "def convert_single_example(tokenizer, example, max_seq_length=256):\r\n",
        "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\r\n",
        "\r\n",
        "    if isinstance(example, PaddingInputExample):\r\n",
        "        input_ids = [0] * max_seq_length\r\n",
        "        input_mask = [0] * max_seq_length\r\n",
        "        segment_ids = [0] * max_seq_length\r\n",
        "        label_ids = [0] * max_seq_length\r\n",
        "        return input_ids, input_mask, segment_ids, label_ids\r\n",
        "    \r\n",
        "    tokens_a = example.text_a\r\n",
        "    if len(tokens_a) > max_seq_length-2:\r\n",
        "        tokens_a = tokens_a[0 : (max_seq_length-2)]\r\n",
        "\r\n",
        "# Token map will be an int -> int mapping between the `orig_tokens` index and\r\n",
        "# the `bert_tokens` index.\r\n",
        "\r\n",
        "# bert_tokens == [\"[CLS]\", \"john\", \"johan\", \"##son\", \"'\", \"s\", \"house\", \"[SEP]\"]\r\n",
        "# orig_to_tok_map == [1, 2, 4, 6]   \r\n",
        "    orig_to_tok_map = []              \r\n",
        "    tokens = []\r\n",
        "    segment_ids = []\r\n",
        "    \r\n",
        "    tokens.append(\"[CLS]\")\r\n",
        "    segment_ids.append(0)\r\n",
        "    orig_to_tok_map.append(len(tokens)-1)\r\n",
        "    #print(len(tokens_a))\r\n",
        "    for token in tokens_a:       \r\n",
        "        tokens.extend(tokenizer.tokenize(token))\r\n",
        "        #tokens.extend(_tokenizer.word_tokenize(token))\r\n",
        "        \r\n",
        "        orig_to_tok_map.append(len(tokens)-1)\r\n",
        "        segment_ids.append(0)\r\n",
        "    tokens.append(\"[SEP]\")\r\n",
        "    segment_ids.append(0)\r\n",
        "    orig_to_tok_map.append(len(tokens)-1)\r\n",
        "    input_ids = tokenizer.convert_tokens_to_ids([tokens[i] for i in orig_to_tok_map])\r\n",
        "    #print(len(orig_to_tok_map), len(tokens), len(input_ids), len(segment_ids)) #for debugging\r\n",
        "\r\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\r\n",
        "    # tokens are attended to.\r\n",
        "    input_mask = [1] * len(input_ids)\r\n",
        "    \r\n",
        "    label_ids = []\r\n",
        "    labels = example.label\r\n",
        "    label_ids.append(0)\r\n",
        "    label_ids.extend([tag2int[label] for label in labels])\r\n",
        "    label_ids.append(0)\r\n",
        "    #print(len(label_ids)) #for debugging\r\n",
        "    # Zero-pad up to the sequence length.\r\n",
        "    while len(input_ids) < max_seq_length:\r\n",
        "        input_ids.append(0)\r\n",
        "        input_mask.append(0)\r\n",
        "        segment_ids.append(0)\r\n",
        "        label_ids.append(0)\r\n",
        "\r\n",
        "    assert len(input_ids) == max_seq_length\r\n",
        "    assert len(input_mask) == max_seq_length\r\n",
        "    assert len(segment_ids) == max_seq_length\r\n",
        "    assert len(label_ids) == max_seq_length\r\n",
        "\r\n",
        "    return input_ids, input_mask, segment_ids, label_ids\r\n",
        "\r\n",
        "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\r\n",
        "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\r\n",
        "\r\n",
        "    input_ids, input_masks, segment_ids, labels = [], [], [], []\r\n",
        "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\r\n",
        "        input_id, input_mask, segment_id, label = convert_single_example(\r\n",
        "            tokenizer, example, max_seq_length\r\n",
        "        )\r\n",
        "        input_ids.append(input_id)\r\n",
        "        input_masks.append(input_mask)\r\n",
        "        segment_ids.append(segment_id)\r\n",
        "        labels.append(label)\r\n",
        "    return (\r\n",
        "        np.array(input_ids),\r\n",
        "        np.array(input_masks),\r\n",
        "        np.array(segment_ids),\r\n",
        "        np.array(labels),\r\n",
        "    )\r\n",
        "\r\n",
        "def convert_text_to_examples(texts, labels):\r\n",
        "    \"\"\"Create InputExamples\"\"\"\r\n",
        "    InputExamples = []\r\n",
        "    for text, label in zip(texts, labels):\r\n",
        "        InputExamples.append(\r\n",
        "            InputExample(guid=None, text_a=text, text_b=None, label=label)\r\n",
        "        )\r\n",
        "    return InputExamples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xaC6FvhqR2o"
      },
      "source": [
        "# tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BB7dl6kv_oR"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShmuNlzSx26e"
      },
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('monsoon-nlp/bert-base-thai')\r\n",
        "from bert1.tokenization import *\r\n",
        "tokenizer = ThaiTokenizer(vocab_file='m.vocab', spm_file='m.model')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbbcPuotm-Hz"
      },
      "source": [
        "custom_words_list = set(thai_words())\r\n",
        "custom_words_list.add('การที่รองอัยการสูงสุด')\r\n",
        "custom_words_list.add('ผบ.ตร.')\r\n",
        "custom_words_list.add('คำสั่งฟ้อง')\r\n",
        "custom_words_list.add('ก.ค.')\r\n",
        "trie = dict_trie(dict_source=custom_words_list)\r\n",
        "_tokenizer = Tokenizer(custom_dict=trie, engine='newmm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmHP70Mynaoq"
      },
      "source": [
        "def bert_labels(labels):\r\n",
        "    train_label_bert = []\r\n",
        "    train_label_bert.append('-PAD-')\r\n",
        "    for i in labels:\r\n",
        "        train_label_bert.append(i)\r\n",
        "    train_label_bert.append('-PAD-')\r\n",
        "    print('BERT labels:', train_label_bert)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xJcwtQxp175"
      },
      "source": [
        "tokens_a = train_text[2]\r\n",
        "tokens_a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3IpLlF9p5yx"
      },
      "source": [
        "\r\n",
        "orig_to_tok_map = []              \r\n",
        "tokens = []\r\n",
        "segment_ids = []\r\n",
        "tokens.append(\"[CLS]\")\r\n",
        "segment_ids.append(0)\r\n",
        "orig_to_tok_map.append(len(tokens)-1)\r\n",
        "for token in tokens_a:\r\n",
        "    #orig_to_tok_map.append(len(tokens)) # keep first piece of tokenized term\r\n",
        "    tokens.extend(tokenizer.tokenize(token))\r\n",
        "    #tokens.extend(_tokenizer.word_tokenize(token))\r\n",
        "    orig_to_tok_map.append(len(tokens)-1) # # keep last piece of tokenized term -->> gives better results!\r\n",
        "    segment_ids.append(0)    \r\n",
        "tokens.append(\"[SEP]\")\r\n",
        "print(tokens)\r\n",
        "segment_ids.append(0)\r\n",
        "print(segment_ids)\r\n",
        "orig_to_tok_map.append(len(tokens)-1)\r\n",
        "print(orig_to_tok_map)\r\n",
        "input_ids = tokenizer.convert_tokens_to_ids([tokens[i] for i in orig_to_tok_map])\r\n",
        "print(input_ids)\r\n",
        "print(len(input_ids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5h3rini26PMy"
      },
      "source": [
        "print('Original tokens:',tokens_a)\r\n",
        "print(len(tokens_a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBLNqZOZEo_s"
      },
      "source": [
        "print('Original tokens:',tokens_a[2])\r\n",
        "print('Original tokens:',train_text[2][2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn3VnU9sDpI_"
      },
      "source": [
        "print(train_text[2])\r\n",
        "print(len(train_text[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGv5lI2DDIB0"
      },
      "source": [
        "print(\"Labal:\",train_label[2])\r\n",
        "print(len(train_label[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIJSvtcj98Ig"
      },
      "source": [
        "print('BERT tokens:',tokens)\r\n",
        "print(len(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HslB9iBW_Kt1"
      },
      "source": [
        "bert_labels(train_label[2])\r\n",
        "print(len(train_label[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwBzWMl3qVLN"
      },
      "source": [
        "print(len(tokens))\r\n",
        "print(len(train_label[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaIMRD3q_QhU"
      },
      "source": [
        "orig_to_tok_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSdmVPka_Tut"
      },
      "source": [
        "print(input_ids)\r\n",
        "print(len(input_ids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foJjlk6E_Wa9"
      },
      "source": [
        "\"\"\"Create InputExamples\"\"\"\r\n",
        "InputExamples = []\r\n",
        "for text, label in zip(train_text[0:1], train_label[0:1]):\r\n",
        "    InputExamples.append(\r\n",
        "        InputExample(guid=None, text_a=text, text_b=None, label=label)\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIqSX7Xc_ZuV"
      },
      "source": [
        "input_ids, input_masks, segment_ids, labels = [], [], [], []\r\n",
        "for example in tqdm_notebook(InputExamples, desc=\"Converting examples to features\"):\r\n",
        "    input_id, input_mask, segment_id, label = convert_single_example(\r\n",
        "        tokenizer, example, MAX_SEQUENCE_LENGTH+2\r\n",
        "    )\r\n",
        "    input_ids.append(input_id)\r\n",
        "    input_masks.append(input_mask)\r\n",
        "    segment_ids.append(segment_id)\r\n",
        "    labels.append(label)\r\n",
        "    print(input_id)\r\n",
        "    print(input_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x8jXcIaIMp8"
      },
      "source": [
        "print(input_id)\r\n",
        "print(len(input_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa5fuib9H3bZ"
      },
      "source": [
        "print(input_ids)\r\n",
        "print(len(input_ids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG3iWFw1JONR"
      },
      "source": [
        "test_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XQ4btTHQkVR"
      },
      "source": [
        "test_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_uZoivY_bLc"
      },
      "source": [
        "train_examples = convert_text_to_examples(train_text, train_label)\r\n",
        "test_examples = convert_text_to_examples(test_text, test_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-e0x71rXTrc"
      },
      "source": [
        "train_label[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzOz9_h6_phs"
      },
      "source": [
        "# Convert to features\r\n",
        "(train_input_ids, train_input_masks, train_segment_ids, train_labels_ids\r\n",
        ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=MAX_SEQUENCE_LENGTH+2)\r\n",
        "(test_input_ids, test_input_masks, test_segment_ids, test_labels_ids\r\n",
        ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=MAX_SEQUENCE_LENGTH+2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kvf_lAsB4eh"
      },
      "source": [
        "print(len(train_input_ids[0]))\r\n",
        "print(len(train_input_masks[0]))\r\n",
        "print(len(train_segment_ids[0]))\r\n",
        "print(len(train_labels_ids[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRGtTVUuCGeQ"
      },
      "source": [
        "train_input_ids[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzQ-23VVCOfX"
      },
      "source": [
        "train_input_masks[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFP9Ae7YCPxn"
      },
      "source": [
        "train_segment_ids[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVcD28gYCRHn"
      },
      "source": [
        "train_labels_ids[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YcFUIdoCSig"
      },
      "source": [
        "# One-hot encode labels\r\n",
        "train_labels = keras.utils.to_categorical(train_labels_ids, num_classes=n_tags)\r\n",
        "test_labels = keras.utils.to_categorical(test_labels_ids, num_classes=n_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IM6URyrCUP4"
      },
      "source": [
        "train_input_ids[0], train_labels[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKJ8m8lHisLw"
      },
      "source": [
        "# bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\r\n",
        "bert_path = \"https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9Lt7qodFtvH"
      },
      "source": [
        "class BertLayer(keras.layers.Layer):\r\n",
        "    def __init__(self, output_representation='sequence_output', trainable=True, **kwargs):\r\n",
        "        self.bert = None\r\n",
        "        super(BertLayer, self).__init__(**kwargs)\r\n",
        "        \r\n",
        "        self.trainable = trainable\r\n",
        "        self.output_representation = output_representation\r\n",
        "\r\n",
        "    def build(self, input_shape):\r\n",
        "        # SetUp tensorflow Hub module\r\n",
        "        self.bert = hub.Module(bert_path,\r\n",
        "                               trainable=self.trainable, \r\n",
        "                               name=\"{}_module\".format(self.name))\r\n",
        "\r\n",
        "        # Assign module's trainable weights to model\r\n",
        "        # Remove unused layers and set trainable parameters\r\n",
        "        # s = [\"/cls/\", \"/pooler/\", 'layer_11', 'layer_10', 'layer_9', 'layer_8', 'layer_7', 'layer_6']\r\n",
        "        s = [\"/cls/\", \"/pooler/\"]\r\n",
        "        self._trainable_weights += [var for var in self.bert.variables[:] if not any(x in var.name for x in s)]\r\n",
        "            \r\n",
        "        for var in self.bert.variables:\r\n",
        "            if var not in self._trainable_weights:\r\n",
        "                self._non_trainable_weights.append(var)\r\n",
        "                \r\n",
        "        # See Trainable Variables\r\n",
        "        #tf.logging.info(\"**** Trainable Variables ****\")\r\n",
        "        #for var in self.trainable_weights:\r\n",
        "        #    init_string = \", *INIT_FROM_CKPT*\"\r\n",
        "        #    tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape, init_string)\r\n",
        "            \r\n",
        "        print('Trainable weights:',len(self._trainable_weights))\r\n",
        "        super(BertLayer, self).build(input_shape)\r\n",
        "\r\n",
        "    def call(self, inputs, mask=None):\r\n",
        "        inputs = [keras.backend.cast(x, dtype=\"int32\") for x in inputs]\r\n",
        "        input_ids, input_mask, segment_ids = inputs\r\n",
        "        bert_inputs = dict(\r\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\r\n",
        "        )\r\n",
        "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\r\n",
        "            self.output_representation\r\n",
        "        ]\r\n",
        "        return result\r\n",
        "\r\n",
        "    def compute_mask(self, inputs, mask=None):\r\n",
        "        return keras.backend.not_equal(inputs[0], 0.0)   \r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        if self.output_representation == 'pooled_output':\r\n",
        "            return (None, 768)\r\n",
        "        else:\r\n",
        "            return (None, None, 768)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJNprEqhFwwp"
      },
      "source": [
        "# Build model\r\n",
        "def build_model(max_seq_length):\r\n",
        "    seed = 0 \r\n",
        "    in_id = keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\r\n",
        "    in_mask = keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\r\n",
        "    in_segment = keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\r\n",
        "    bert_inputs = [in_id, in_mask, in_segment]\r\n",
        "    \r\n",
        "    np.random.seed(seed)\r\n",
        "    bert_output = BertLayer()(bert_inputs)\r\n",
        "\r\n",
        "    np.random.seed(seed)\r\n",
        "    outputs = keras.layers.Dense(n_tags, activation=keras.activations.softmax)(bert_output)\r\n",
        "\r\n",
        "    np.random.seed(seed)\r\n",
        "    model = keras.models.Model(inputs=bert_inputs, outputs=outputs)\r\n",
        "    np.random.seed(seed)\r\n",
        "    model.compile(optimizer=keras.optimizers.Adam(lr=0.00004), loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])   \r\n",
        "    model.summary(100)\r\n",
        "    return model\r\n",
        "\r\n",
        "def initialize_vars(sess):\r\n",
        "    sess.run(tf.local_variables_initializer())\r\n",
        "    sess.run(tf.global_variables_initializer())\r\n",
        "    sess.run(tf.tables_initializer())\r\n",
        "    keras.backend.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeHMULMeHgI-"
      },
      "source": [
        "# import tf_sentencepiece\r\n",
        "model = build_model(MAX_SEQUENCE_LENGTH+2) # We sum 2 for [CLS], [SEP] tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-CDL56QHh41"
      },
      "source": [
        "keras.utils.plot_model(model, to_file='model.png', show_shapes=True)\r\n",
        "Image('model.png')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQnq19wUJSzR"
      },
      "source": [
        "train_input_ids.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70IBtwXTJxSK"
      },
      "source": [
        "train_input_masks.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05mlnjpsJzMA"
      },
      "source": [
        "train_segment_ids.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCqxljYxJ0iY"
      },
      "source": [
        "train_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv47-W65J10X"
      },
      "source": [
        "initialize_vars(sess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qASrZnxJ_HA"
      },
      "source": [
        "t_ini = datetime.datetime.now()\r\n",
        "print(EPOCHS)\r\n",
        "EPOCHS=30\r\n",
        "cp = keras.callbacks.ModelCheckpoint(filepath=\"model.ckpt\",\r\n",
        "                     monitor='val_acc',\r\n",
        "                     save_best_only=True,\r\n",
        "                     save_weights_only=True,\r\n",
        "                     verbose=1)\r\n",
        "\r\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor = 'val_acc', patience = 5)\r\n",
        "\r\n",
        "history = model.fit([train_input_ids, train_input_masks, train_segment_ids], \r\n",
        "                    train_labels,\r\n",
        "                    validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\r\n",
        "                    #validation_split=0.3,\r\n",
        "                    epochs=EPOCHS,\r\n",
        "                    batch_size=16,\r\n",
        "                    shuffle=True,\r\n",
        "                    verbose=1,\r\n",
        "                    callbacks=[cp, early_stopping]\r\n",
        "                   ) \r\n",
        "\r\n",
        "t_fin = datetime.datetime.now()\r\n",
        "print('Training completed in {} seconds'.format((t_fin - t_ini).total_seconds()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-f_ui0rKXto"
      },
      "source": [
        "model = build_model(MAX_SEQUENCE_LENGTH+2) \r\n",
        "model.load_weights('model.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNtoH40JSa3s"
      },
      "source": [
        "y_pred = model.predict([test_input_ids, test_input_masks, test_segment_ids]).argmax(-1)\r\n",
        "y_true = test_labels.argmax(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMt0rbcqSl54"
      },
      "source": [
        "y_true[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn3xHrAuSnKp"
      },
      "source": [
        "def y2label(zipped, mask=0):\r\n",
        "    out_true = []\r\n",
        "    out_pred = []\r\n",
        "    for zip_i in zipped:\r\n",
        "        a, b = tuple(zip_i)\r\n",
        "        if a != mask :\r\n",
        "            out_true.append(int2tag[a])\r\n",
        "            out_pred.append(int2tag[b])\r\n",
        "    return out_true, out_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s6FsqDCSpXx"
      },
      "source": [
        "y_zipped = zip(y_true.flat, y_pred.flat)\r\n",
        "y_true, y_pred = y2label(y_zipped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNRGE5FcSvMw"
      },
      "source": [
        "len(y_true), len(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLfauszeSw4A"
      },
      "source": [
        "name='Bert fine-tuned model'\r\n",
        "print('\\n------------ Result of {} ----------\\n'.format(name))\r\n",
        "print(classification_report(y_true, y_pred, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL0REOtHS0KQ"
      },
      "source": [
        "print(\"Accuracy: {0:.4f}\".format(accuracy_score(y_true, y_pred)))\r\n",
        "print('f1-macro score: {0:.4f}'.format(f1_score(y_true, y_pred, average='macro')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLMoeWKAS_cP"
      },
      "source": [
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\r\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\r\n",
        "    plt.title(title)\r\n",
        "    if ylim is not None:\r\n",
        "        plt.ylim(*ylim)\r\n",
        "    plt.xlabel(\"Training examples\")\r\n",
        "    plt.ylabel(\"Accuracy\")\r\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\r\n",
        "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='accuracy')\r\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\r\n",
        "    train_scores_std = np.std(train_scores, axis=1)\r\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\r\n",
        "    test_scores_std = np.std(test_scores, axis=1)\r\n",
        "    plt.grid()\r\n",
        "\r\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\r\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\r\n",
        "                     color=\"r\")\r\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\r\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\r\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\r\n",
        "             label=\"Training score\")\r\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\r\n",
        "             label=\"Cross-validation score\")\r\n",
        "\r\n",
        "    plt.legend(loc=\"best\")\r\n",
        "    return plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9AbEBUxTDjX"
      },
      "source": [
        "def plot_confusion_matrix(f1,\r\n",
        "                          cm,\r\n",
        "                          target_names,\r\n",
        "                          title='Confusion matrix',\r\n",
        "                          cmap=None,\r\n",
        "                          normalize=True,\r\n",
        "                          i=1):\r\n",
        "    \"\"\"\r\n",
        "    given a sklearn confusion matrix (cm), make a nice plot\r\n",
        "\r\n",
        "    Arguments\r\n",
        "    ---------\r\n",
        "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\r\n",
        "\r\n",
        "    target_names: given classification classes such as [0, 1, 2]\r\n",
        "                  the class names, for example: ['high', 'medium', 'low']\r\n",
        "\r\n",
        "    title:        the text to display at the top of the matrix\r\n",
        "\r\n",
        "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\r\n",
        "                  see http://matplotlib.org/examples/color/colormaps_reference.html\r\n",
        "                  plt.get_cmap('jet') or plt.cm.Blues\r\n",
        "\r\n",
        "    normalize:    If False, plot the raw numbers\r\n",
        "                  If True, plot the proportions\r\n",
        "\r\n",
        "    Usage\r\n",
        "    -----\r\n",
        "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\r\n",
        "                                                              # sklearn.metrics.confusion_matrix\r\n",
        "                          normalize    = True,                # show proportions\r\n",
        "                          target_names = y_labels_vals,       # list of names of the classes\r\n",
        "                          title        = best_estimator_name) # title of graph\r\n",
        "\r\n",
        "    Citiation\r\n",
        "    ---------\r\n",
        "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    import matplotlib.pyplot as plt\r\n",
        "    import numpy as np\r\n",
        "    import itertools\r\n",
        "\r\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\r\n",
        "    misclass = 1 - accuracy\r\n",
        "    plt.figure(figsize=(20, 12))\r\n",
        "    if cmap is None:\r\n",
        "        cmap = plt.get_cmap('Blues')\r\n",
        "\r\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\r\n",
        "    plt.title(title)\r\n",
        "    plt.colorbar()\r\n",
        "\r\n",
        "    if target_names is not None:\r\n",
        "        tick_marks = np.arange(len(target_names))\r\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\r\n",
        "        plt.yticks(tick_marks, target_names)\r\n",
        "\r\n",
        "    if normalize:\r\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\r\n",
        "\r\n",
        "\r\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\r\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n",
        "        if normalize:\r\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\r\n",
        "                     horizontalalignment=\"center\",\r\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\r\n",
        "        else:\r\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\r\n",
        "                     horizontalalignment=\"center\",\r\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\r\n",
        "\r\n",
        "\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.ylabel('True label')\r\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}; f1-score={:0.4f}'.format(accuracy, misclass, f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SipSU2KqTM93"
      },
      "source": [
        "def plot_acc():\r\n",
        "    plt.plot(history.history['acc'])\r\n",
        "    plt.plot(history.history['val_acc'])\r\n",
        "    plt.title('model accuracy')\r\n",
        "    plt.ylabel('accuracy')\r\n",
        "    plt.xlabel('epoch')\r\n",
        "    plt.legend(['train', 'test'], loc='upper left')\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9CWsUPDTOwu"
      },
      "source": [
        "def plot_loss():\r\n",
        "    plt.plot(history.history['loss'])\r\n",
        "    plt.plot(history.history['val_loss'])\r\n",
        "    plt.title('model loss')\r\n",
        "    plt.ylabel('loss')\r\n",
        "    plt.xlabel('epoch')\r\n",
        "    plt.legend(['train', 'test'], loc='upper left')\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64jx5KjyTQVH"
      },
      "source": [
        "tags = sorted(set(y_pred+y_true))\r\n",
        "cnf_matrix = confusion_matrix(y_true, y_pred)\r\n",
        "plot_confusion_matrix(f1_score(y_true, y_pred, average='macro'), cnf_matrix, target_names=tags, title=name, normalize=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNVGd0K4TSvY"
      },
      "source": [
        "y_pred = model.predict([test_input_ids, test_input_masks, test_segment_ids], batch_size=16).argmax(-1)\r\n",
        "y_true = test_labels_ids\r\n",
        "\r\n",
        "def make_prediction(i=16):\r\n",
        "    note = ''\r\n",
        "    sent = []\r\n",
        "    print(\"{:10} {:5} : {:5}\".format(\"Word\", \"True\", \"Predicted\"))\r\n",
        "    print(35*'-')\r\n",
        "    for w, true, pred in zip(test_input_ids[i], y_true[i], y_pred[i]):\r\n",
        "        if tokenizer.convert_ids_to_tokens([w])[0]!='[PAD]' and \\\r\n",
        "            tokenizer.convert_ids_to_tokens([w])[0]!='[CLS]' and \\\r\n",
        "            tokenizer.convert_ids_to_tokens([w])[0]!='[SEP]':\r\n",
        "            if int2tag[true] != int2tag[pred]: note='<<--- Error!'\r\n",
        "            print(\"{:10} {:5} : {:5} {:5}\".format(tokenizer.convert_ids_to_tokens([w])[0], int2tag[true], int2tag[pred], note))\r\n",
        "            note=''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7GVrLNVT2Xu"
      },
      "source": [
        "make_prediction(i=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9rYlCsBT7Ym"
      },
      "source": [
        "sentence_raw1 = 'พี่สาวผงะ พบโครงกระดูกติดซากรถถูกไฟคลอกดับ เชื่อเป็นน้องชายที่หายไปนั้น ได้รับรายงานจากการ'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgwJEMORSEB8"
      },
      "source": [
        "sentence_raw=sentence_raw1.replace(\" \", \"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0l74TIqUGAu"
      },
      "source": [
        "# from pythainlp.tokenize import word_tokenize\r\n",
        "sentence_ini = word_tokenize(sentence_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEe-4tIqwu_k"
      },
      "source": [
        "sentence_ini"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtC8S69OUb6l"
      },
      "source": [
        "tokens_a = sentence_ini"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_74GMIqUkJt"
      },
      "source": [
        "orig_to_tok_map = []              \r\n",
        "tokens = []\r\n",
        "segment_ids = []\r\n",
        "tokens.append(\"[CLS]\")\r\n",
        "segment_ids.append(0)\r\n",
        "orig_to_tok_map.append(len(tokens)-1)\r\n",
        "for token in tokens_a:\r\n",
        "    #orig_to_tok_map.append(len(tokens)) # keep first piece of tokenized term\r\n",
        "    tokens.extend(tokenizer.tokenize(token))\r\n",
        "    # tokens.extend(_tokenizer.word_tokenize(token))\r\n",
        "    orig_to_tok_map.append(len(tokens)-1) # # keep last piece of tokenized term -->> gives better results!\r\n",
        "    segment_ids.append(0)\r\n",
        "tokens.append(\"[SEP]\")\r\n",
        "segment_ids.append(0)\r\n",
        "orig_to_tok_map.append(len(tokens)-1)\r\n",
        "input_ids = tokenizer.convert_tokens_to_ids([tokens[i] for i in orig_to_tok_map])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn7tJOK9Uo46"
      },
      "source": [
        "# print('Original tokens:',tokens_a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mic2WpkhUq0l"
      },
      "source": [
        "print('BERT tokens:',tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVYJD2RPUtgV"
      },
      "source": [
        "orig_to_tok_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAKZCHBlU3PE"
      },
      "source": [
        "[tokens[i] for i in orig_to_tok_map]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbtDrnEQU5Fs"
      },
      "source": [
        "test_example = convert_text_to_examples([sentence_ini], [['-PAD-']*len(sentence_ini)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUrsQKD18jq_"
      },
      "source": [
        "# Convert to features\r\n",
        "(input_ids, input_masks, segment_ids, train_labels_ids\r\n",
        ") = convert_examples_to_features(tokenizer, test_example, max_seq_length=MAX_SEQUENCE_LENGTH+2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_nA99QL8mVP"
      },
      "source": [
        "predictions = model.predict([input_ids, input_masks, segment_ids], batch_size=1).argmax(-1)[0]\r\n",
        "print(\"\\n{:20}| {:15}: {:15}\".format(\"Word in BERT layer\", 'Initial word', \"Predicted POS-tag\"))\r\n",
        "print(61*'-')\r\n",
        "k = 0\r\n",
        "for i, pred in enumerate(predictions):\r\n",
        "    try:\r\n",
        "        if pred!=0:\r\n",
        "            print(\"{:20}| {:15}: {:15}\".format([tokens[i] for i in orig_to_tok_map][i], sentence_ini[i-1], int2tag[pred]))            \r\n",
        "            k+=1            \r\n",
        "    except:\r\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIlE2zs9k0oJ"
      },
      "source": [
        "def find_errors(X,y):\r\n",
        "    error_counter = collections.Counter()\r\n",
        "    support = 0\r\n",
        "    for i in range(test_input_ids.shape[0]):\r\n",
        "        for w, true, pred in zip(test_input_ids[i], y_true[i], y_pred[i]):\r\n",
        "            if int2tag[true]!='-PAD-':\r\n",
        "                if true != pred:\r\n",
        "                    word = tokenizer.convert_ids_to_tokens([w])[0]\r\n",
        "                    error_counter[word] += 1\r\n",
        "                support += 1\r\n",
        "    return error_counter, support"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekBt3VDjJYwX"
      },
      "source": [
        "errors, support = find_errors([test_input_ids],y_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVM2wfqjJazJ"
      },
      "source": [
        "print('Total errors/Total words: {}/{} | Accuracy: {:.4}\\n'.format(sum(errors.values()), support, 1-sum(errors.values())/support))\r\n",
        "print('Most common errors:', errors.most_common(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jubb6vZXJcYi"
      },
      "source": [
        "plot_acc()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUba1YXaJdsl"
      },
      "source": [
        "plot_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ume9uKeuBxG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}